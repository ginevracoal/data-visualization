---
title: "trump"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(ggplot2)
# get a reference to the table songs
plotCountBy<-function(songs,groupName){
  # run a query in dplyr, count number of songs for each year
  songsByGroup<-songs %>% 
    group_by(groupValue) %>%
    summarise(count = n()) %>% 
    collect()
  
  ggplot(songsByGroup,aes(x=groupValue,y=count))+
    geom_col()+
    xlab(groupName)
}


library(readr)
articles<-read_csv("./2017.csv",col_names = c("id", "data","number_article","number_edition"), col_types="ncnn", skip=1)

articles$data<-gsub("Get 3 free articles per week, daily \r\n newsletters and more.","",articles$data)
```



Analizziamo il numero di parole usato in ciascuna articolo nel corso delle edizioni
```{r fig.width=10}
library(plotly)
library(tidytext)
library(tm)
library(tidyr)

# spezza il testo in singole parole
tokens<-articles %>% mutate(data=removePunctuation(data)) %>%  unnest_tokens(word, data) %>% ungroup()


# considera a gruppo l'articolo
# per ogni articolo quante volte è apparsa ciascuna parola
words<-tokens %>% group_by(id,word) %>% summarise(count=n())

```

```{r}
# calcola numero parole totali per articolo
totalWords<-words %>% group_by(id) %>% summarise(totalCount=sum(count))

data<-articles %>% left_join(totalWords,by=c("id"="id"))

# some articles are wrong, less than 100 total words
data<-data %>% filter(totalCount>100)



data<- data %>% left_join(words %>% filter(word=="trump" | word == "clinton"),by=c("id"="id")) %>% select(-data)

ggplot(data,aes(x=id,y=count,fill=word))+
    geom_col()


```

```{r}
ggplot(data %>%filter(!is.na(word)) %>%  mutate(count=min(count,1)),aes(x=id,y=count,fill=word))+
    geom_col()
```


```{r}
trumpClinton_edition<-data %>% filter(!is.na(word))%>% group_by(number_edition,word) %>% summarise(count=sum(count))
ggplot(trumpClinton_edition,aes(x=number_edition,y=count,fill=word))+
    geom_col()
```


```{r}
ggplot(trumpClinton_edition %>% mutate(count=min(count,1)),aes(x=number_edition,y=count,fill=word))+
    geom_col()
```


Come sopra ma eliminando le stop words
```{r}
# rimuovi stop words
stop_words<-data.frame(word=stopwords("en"))
tokens_without_stop_words<-tokens %>% anti_join(stop_words)

words_orig<-tokens_without_stop_words %>% group_by(id,word) %>% summarise(count=n())

# calcola numero parole totali per articolo
totalWords<-words_orig %>% group_by(id) %>% summarise(totalCount_withoutStopWords=sum(count))

data<-data %>% left_join(totalWords,by=c("id"="id"))

```

Considera uso delle parole
```{r}
# quante volte è stata usata ciascuna parola e in quanti articoli
words<-tokens_without_stop_words %>%  group_by(word) %>% summarise(count=n(),articles=length(unique(id))) %>% arrange(desc(count))

# word cloud
library(wordcloud)
# in base a quante volte è stata usata una parola
wordcloud(words$word,words$count,max.words = 50)
```


```{r}
# in base a quanti articoli hanno usato una parola
wordcloud(words$word,words$articles,max.words = 50)

```
Considera uso delle parole applicando lo stemming
```{r}
library(SnowballC)
print(getStemLanguages())
# quante volte è stata usata ciascuna parola e in quante canzoni
words<-tokens_without_stop_words %>% ungroup %>% mutate(word=wordStem(word,language="english"))%>% group_by(word) %>%  summarise(count=n(),articles=length(unique(id))) %>% arrange(desc(count))

# word cloud
wordcloud(words$word,words$count,max.words = 50)

```

Considera tf-idf delle parole
```{r}
words<-words_orig %>%  bind_tf_idf(word, id, count) %>% arrange(-tf_idf) %>% top_n(30)

wordcloud(words$word,words$tf_idf,max.words = 30)
```

Considera co-occorenza delle parole

```{r}
#quante volte è stata usata una parola in ciasscuna canzone
words<-tokens_without_stop_words %>%  group_by(word) %>% summarise(count=n(),articles=length(unique(id))) %>% arrange(desc(count))

```
co occorenze delle parole

https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html#2_counting_co-occurrences

```{r}
who<-"clinton"  # specify the articles of whom
mainWord<-"clinton"  # specify the word to consider as the main one


trumpArticles<-data %>% filter(word==who & count>0) %>% inner_join(articles,by=c("id","id"))
text<-trumpArticles$data %>% tolower

# limit number of articles
text<-text[1:100]


ds  <- Corpus(VectorSource(text))
binDTM <- DocumentTermMatrix(ds, control=list(bounds = list(global=c(1, Inf)), weighting = weightBin))
require(Matrix)
binDTM <- sparseMatrix(i = binDTM$i, j = binDTM$j, x = binDTM$v, dims = c(binDTM$nrow, binDTM$ncol), dimnames = dimnames(binDTM))

# Matrix multiplication for cooccurrence counts
coocCounts <- t(binDTM) %*% binDTM


cc<-as.matrix(coocCounts)
```

Trova co occorrenze più frequenti
```{r}
diag(cc)<-0# sulla diagonale si ha il nuero di volte che è comparsa una certa parola
freq<-sort(cc,decreasing = TRUE)
#sapply(freq[1:20],function(x){which(cc==x,arr.ind=TRUE)  %>% rownames})

# elimina parole poco usate
cwords<-words$wordolnames(cc)%in%cwords)
cc_s<-cc[ic,ic]
freq<-order(cc_s,decreasing = TRUE)
words<-colnames(cc_s)
#co_occ<-lapply(freq[1:50],function(x){return(c(words[floor(x/nrow(cc_s))],words[x%%nrow(cc_s)]))})

co_occ<-sapply(freq[1:50],function(x){return(paste(words[floor(x/nrow(cc_s))],words[x%%nrow(cc_s)],sep="-"))})

d<-data.frame(a=co_occ) %>% separate(a,sep = "-",into=c("a","b"))
l<-(d %>% group_by(a) %>% summarise(count=n())) %>% rename(w=a)
r<-(d %>% group_by(b) %>% summarise(count=n())) %>% rename(w=b)
co_occ<-l %>% rbind( r) %>% arrange(desc(count))
```


```{r}

calculateCoocStatistics<-function(coocTerm,binDTM,measure){
  k <- nrow(binDTM)
  ki <- sum(binDTM[, coocTerm])
  kj <- colSums(binDTM)
  names(kj) <- colnames(binDTM)
  kij <- coocCounts[coocTerm, ]
  
  if(measure=="mutual"){
  ########## MI: log(k*kij / (ki * kj) ########
    mutualInformationSig <- log(k * kij / (ki * kj))
    mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]
    return(mutualInformationSig)
  }else if (measure=="dice"){
    ########## DICE: 2 X&Y / X + Y ##############
    dicesig <- 2 * kij / (ki + kj)
    dicesig <- dicesig[order(dicesig, decreasing=TRUE)]
    return(dicesig)
  }else{
    ########## Log Likelihood ###################
    logsig <- 2 * ((k * log(k)) - (ki * log(ki)) - (kj * log(kj)) + (kij * log(kij)) 
              + (k - ki - kj + kij) * log(k - ki - kj + kij) 
              + (ki - kij) * log(ki - kij) + (kj - kij) * log(kj - kij) 
              - (k - ki) * log(k - ki) - (k - kj) * log(k - kj))
    logsig <- logsig[order(logsig, decreasing=T)]
    return(logsig)
  }
}
```


```{r}
term<-mainWord
kij <- coocCounts[term, ]
mutualInformationSig<-calculateCoocStatistics(term,binDTM,"mutual")
dicesig<-calculateCoocStatistics(term,binDTM,"dice")
logsig<-calculateCoocStatistics(term,binDTM,"mutual")
# Put all significance statistics in one Data-Frame
resultOverView <- data.frame(
  names(sort(kij, decreasing=T)[1:10]), sort(kij, decreasing=T)[1:10],
  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], 
  names(dicesig[1:10]), dicesig[1:10], 
  names(logsig[1:10]), logsig[1:10],
  row.names = NULL)
colnames(resultOverView) <- c("Freq-terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LL-Terms", "LL")
print(resultOverView)
```

```{r}
coocTerm<-mainWord # specify the word to consider as the main one
coocs <- calculateCoocStatistics(term, binDTM, measure="LOGLIK")
numberOfCoocs<-15
resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
# The structure of the temporary graph object is equal to that of the resultGraph
tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
tmpGraph[, 1] <- coocTerm
# Entry of the co-occurrences into the second column of the respective line
tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]
# Set the significances
tmpGraph[, 3] <- coocs[1:numberOfCoocs]

# Attach the triples to resultGraph
resultGraph <- rbind(resultGraph, tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs){
  
  # Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newCoocTerm <- names(coocs)[i]
  coocs2 <- calculateCoocStatistics(newCoocTerm, binDTM, measure="LOGLIK")
  
  #print the co-occurrences
  coocs2[1:10]
  
  # Structure of the temporary graph object
  tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]
  tmpGraph[, 1] <- newCoocTerm
  tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]
  tmpGraph[, 3] <- coocs2[1:numberOfCoocs]
  
  #Append the result to the result graph
  resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])
}

resultGraph[sample(nrow(resultGraph), 6), ]
```


```{r}
require(igraph)

# Set the graph and type. In this case, "F" means "Force Directed"
graphNetwork <- graph.data.frame(resultGraph, directed = F)

# Identification of all nodes with less than 2 edges
graphVs <- V(graphNetwork)[degree(graphNetwork) < 2]
# These edges are removed from the graph
graphNetwork <- delete.vertices(graphNetwork, graphVs) 

# Assign colors to edges and nodes (searchterm blue, rest orange)
V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, 'cornflowerblue', 'orange') 

# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange
halfMaxSig <- max(E(graphNetwork)$sig) * 0.5
E(graphNetwork)$color <- ifelse(E(graphNetwork)$sig > halfMaxSig, "coral", "azure3")

# Disable edges with radius
E(graphNetwork)$curved <- 0 
# Size the nodes by their degree of networking
V(graphNetwork)$size <- log(degree(graphNetwork)) * 5

# All nodes must be assigned a standard minimum-size
V(graphNetwork)$size[V(graphNetwork)$size < 5] <- 3 

# edge thickness
E(graphNetwork)$width <- 2

# Define the frame and spacing for the plot
par(mai=c(0,0,1,0)) 

# Finaler Plot
plot(graphNetwork,              
     layout = layout.fruchterman.reingold,  # Force Directed Layout 
     main = paste(coocTerm, ' Graph'),
     vertex.label.family = "sans",
     vertex.label.cex = 0.8,
     vertex.shape = "circle",
     vertex.label.dist = 0.5,           # Labels of the nodes moved slightly
     vertex.frame.color = 'darkolivegreen',
     vertex.label.color = 'black',      # Color of node names
     vertex.label.font = 2,         # Font of node names
     vertex.label = V(graphNetwork)$name,       # node names
     vertex.label.cex = 1 # font size of node names 
)
##########
#ggraph(graphNetwork)+
#  geom_node_point()+
#  geom_node_text()+
#  geom_edge_link()
```