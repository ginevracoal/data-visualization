---
title: "trump"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(ggplot2)
# get a reference to the table songs
plotCountBy<-function(songs,groupName){
  # run a query in dplyr, count number of songs for each year
  songsByGroup<-songs %>% 
    group_by(groupValue) %>%
    summarise(count = n()) %>% 
    collect()
  
  ggplot(songsByGroup,aes(x=groupValue,y=count))+
    geom_col()+
    xlab(groupName)
}


library(readr)
library(feather)
articles<-read_feather(path="my_data2.feather")
articles$id<-seq(1,nrow(articles))
#articles<-read_csv("./2017.csv",col_names = c("id", "data","number_article","number_edition"), col_types="ncnn", skip=1)

articles$article<-gsub("Get 3 free articles per week, daily \n newsletters and more.","",articles$article)

# some articles are wrong, less than 166 characters
articles<-articles %>% filter(nchar(article)>166)
```



Analizziamo il numero di parole usato in ciascuna articolo nel corso delle edizioni
```{r fig.width=10}
library(plotly)
library(tidytext)
library(tm)
library(tidyr)

# spezza il testo in singole parole
tokens<-articles %>% mutate(article=removePunctuation(article)) %>%  unnest_tokens(word, article) %>% ungroup()


# considera a gruppo l'articolo
# per ogni articolo quante volte è apparsa ciascuna parola
words<-tokens %>% group_by(id,word) %>% summarise(count=n())

```

```{r}
# calcola numero parole totali per articolo
totalWords<-words %>% group_by(id) %>% summarise(totalCount=sum(count))

data<-articles %>% left_join(totalWords,by=c("id"="id"))




articleFor<-words %>% filter(word=="trump" | word == "clinton") %>% mutate(count=!is.na(count))%>% spread(word,count) %>% mutate(both=!is.na(clinton)&!is.na(trump))
data<- data %>% left_join(articleFor,by=c("id"="id")) %>% select(-article)

data<-data %>% select(id,trump,clinton,both) %>% gather(who,present,-id) %>% filter(present==TRUE)

data<-data %>% left_join(articles,by=c("id"="id"))


# add column with boolean before or after election
data$beforeEl<-as.Date(data$date)<as.Date("2016-11-08")

library(lubridate)
data$datew<-floor_date(as.Date(data$date),  unit="week")

ggplot(data,aes(x=date,y=1,fill=who))+
    geom_col()


```

```{r}
#ggplot(data %>%filter(!is.na(word)) %>%  mutate(count=min(count,1)),aes(x=id,y=count,fill=word))+
 #   geom_col()
```


```{r}
trumpClinton_edition<-data %>% group_by(datew,who) %>% summarise(count=n())
ggplot(trumpClinton_edition,aes(x=datew,y=count,fill=who))+
    geom_col()
```



Come sopra ma eliminando le stop words
```{r}
# rimuovi stop words
stop_words<-data.frame(word=stopwords("en"))
tokens_without_stop_words<-tokens %>% anti_join(stop_words)

words_orig<-tokens_without_stop_words %>% group_by(id,word) %>% summarise(count=n())

# calcola numero parole totali per articolo
totalWords<-words_orig %>% group_by(id) %>% summarise(totalCount_withoutStopWords=sum(count))

data<-data %>% left_join(totalWords,by=c("id"="id"))

```



Considera uso delle parole
```{r}
# quante volte è stata usata ciascuna parola e in quanti articoli
words<-tokens_without_stop_words %>%  group_by(word) %>% summarise(count=n(),articles=length(unique(id))) %>% arrange(desc(count))

# word cloud
library(wordcloud)
# in base a quante volte è stata usata una parola
wordcloud(words$word,words$count,max.words = 50)
```


```{r}
wordcloud(words$word,words$articles,max.words = 50)
```

```{r}
getArtsOf<-function(data,name){
  arts_both<-data %>% filter((who=="both" & present))
  if(name=="both"){
    arts<-arts_both
  }else{
arts<-data %>% filter((who==name & present)) %>% anti_join(arts_both,by=c("id","id"))
}

arts$datew<-floor_date(as.Date(arts$date), unit="week")
return(arts)
}
getWordsOf<-function(arts){

# spezza il testo in singole parole
tokens<-arts %>% mutate(article=removePunctuation(article)) %>%  unnest_tokens(word, article) %>% ungroup()


# considera a gruppo l'articolo
# per ogni articolo quante volte è apparsa ciascuna parola
words<-tokens %>% group_by(id,word) %>% summarise(count=n())


# rimuovi stop words
stop_words<-data.frame(word=stopwords("en"))
tokens_without_stop_words<-tokens %>% anti_join(stop_words)

words_orig<-tokens_without_stop_words %>% group_by(id,word) %>% summarise(count=n())
words_orig<-words_orig %>% left_join(arts,by=c("id"="id")) %>% ungroup()
return(words_orig)

}


export_csv_data<-function(name,words_orig,words_owners,half_diff_n,total_n){
words_orig<-words_orig %>% group_by(datew,word) %>% summarise(count=sum(count))

#tot<-words_orig %>% ungroup() %>% group_by(word) %>% summarise(total=sum(count)) %>% select(word,total)

tfw<-words_orig %>%  bind_tf_idf(word, datew, count)

# add column with boolean before or after election
tfw$beforeEl<-as.Date(tfw$datew)<as.Date("2016-11-08")

# check words used before and not after and viceversa
t<-tfw %>% group_by(word,beforeEl) %>% summarise(occ=n(),oc=n(),count = mean(count),tf_idf=mean(tf_idf),tf=mean(tf)) 

f<-t %>% spread(beforeEl,oc) %>% rename(before=`TRUE`,after=`FALSE`)
ff<-f %>% group_by(word) %>% summarise(occ_m=min(occ),occ_M=max(occ),tf_m=min(tf),tf_M=max(tf),before=sum(before,na.rm=T),after=sum(after,na.rm=T))
ff$diff<-ff$after-ff$before

words_diff<-ff %>% select(word,diff)

most_before<-ff %>% top_n(half_diff_n,-diff) 
most_after<-ff %>% top_n(half_diff_n,diff)



words_ids<-tfw %>% group_by(word) %>% summarise(datews = paste0(datew, collapse = "|"),tf_idf=mean(tf_idf),tf=mean(tf)) 

#words_ids<-words_ids %>% left_join(tot,by=c("word","word")) 
#words_ids<-words_ids%>% mutate(cf=count/total)
words_ids<-words_ids %>% ungroup()

res<-list()

wo<-words_ids %>% top_n(total_n,tf)

# transform range of tf and tf_idf in [1,100]
wo$tf<-1+100*(wo$tf-min(wo$tf))/(max(wo$tf-min(wo$tf)))
wo$tf_idf<-1+100*(wo$tf_idf-min(wo$tf_idf))/(max(wo$tf_idf-min(wo$tf_idf)))

wo<-wo %>% left_join(words_owners,by=c("word","word"))
wo<-wo %>% left_join(words_diff,by=c("word","word"))
wo$n_weeks<-nchar(wo$datews)/10
res[["tf"]]<-wo


wo<-words_ids %>% top_n(total_n,tf_idf)

# transform range of tf and tf_idf in [1,100]
wo$tf<-1+100*(wo$tf-min(wo$tf))/(max(wo$tf-min(wo$tf)))
wo$tf_idf<-1+100*(wo$tf_idf-min(wo$tf_idf))/(max(wo$tf_idf-min(wo$tf_idf)))


wo<-wo %>% left_join(words_owners,by=c("word","word"))
wo<-wo %>% left_join(words_diff,by=c("word","word"))
wo$n_weeks<-nchar(wo$datews)/10
res[["tfidf"]]<-wo

wb<-most_before %>% select(word) %>% left_join(words_ids)
wa<-most_after %>% select(word) %>% left_join(words_ids)
wo<-bind_rows(wb,wa)


# transform range of tf and tf_idf in [1,100]
wo$tf<-1+100*(wo$tf-min(wo$tf))/(max(wo$tf-min(wo$tf)))
wo$tf_idf<-1+100*(wo$tf_idf-min(wo$tf_idf))/(max(wo$tf_idf-min(wo$tf_idf)))


wo<-wo %>% left_join(words_owners,by=c("word","word"))
wo<-wo %>% left_join(words_diff,by=c("word","word"))
wo$n_weeks<-nchar(wo$datews)/10
res[["bef_aft"]]<-wo
return(res)
}
cl_art<-getArtsOf(data,"clinton")
tr_art<-getArtsOf(data,"trump")

both_art<-getArtsOf(data,"both") %>% bind_rows(cl_art) %>% bind_rows(tr_art)



cl<-getWordsOf(cl_art)
tr<-getWordsOf(tr_art)
both_words<-getWordsOf(both_art)

cl_not_tr<-cl %>% distinct(word) %>% anti_join(tr,by=c("word"="word")) %>% mutate(owner="clinton")
tr_not_cl<-tr %>% distinct(word) %>% anti_join(cl,by=c("word"="word")) %>% mutate(owner="trump")
cl_tr_words<-cl_not_tr %>% bind_rows(tr_not_cl) %>% ungroup() %>% select(word,owner)


name<-"clinton"
res<-export_csv_data(name,cl,cl_tr_words,10,1000)
write_csv(res[["tf"]],paste0(name,"words_articles_tf.csv"))
write_csv(res[["tfidf"]],paste0(name,"words_articles_tfidf.csv"))
write_csv(res[["bef_aft"]],paste0(name,"words_bef_aft.csv"))

name<-"trump"
res<-export_csv_data(name,tr,cl_tr_words,10,1000)
write_csv(res[["tf"]],paste0(name,"words_articles_tf.csv"))
write_csv(res[["tfidf"]],paste0(name,"words_articles_tfidf.csv"))
write_csv(res[["bef_aft"]],paste0(name,"words_bef_aft.csv"))


name<-"both"
res<-export_csv_data(name,both_words,cl_tr_words,10,1000)
write_csv(res[["tf"]],paste0(name,"words_articles_tf.csv"))
write_csv(res[["tfidf"]],paste0(name,"words_articles_tfidf.csv"))
write_csv(res[["bef_aft"]],paste0(name,"words_bef_aft.csv"))


```




```{r}
# in base a quanti articoli hanno usato una parola
wordcloud(words$word,words$articles,max.words = 50)

```
Considera uso delle parole applicando lo stemming
```{r}
library(SnowballC)
print(getStemLanguages())
# quante volte è stata usata ciascuna parola e in quante canzoni
words<-tokens_without_stop_words %>% ungroup %>% mutate(word=wordStem(word,language="english"))%>% group_by(word) %>%  summarise(count=n(),articles=length(unique(id))) %>% arrange(desc(count))

# word cloud
wordcloud(words$word,words$count,max.words = 50)

```

Considera tf-idf delle parole
```{r}
words<-words_orig %>%  bind_tf_idf(word, id, count) %>% arrange(-tf_idf) %>% top_n(30)

wordcloud(words$word,words$tf_idf,max.words = 30)
```

Considera co-occorenza delle parole

```{r}
#quante volte è stata usata una parola in ciasscuna canzone
words<-tokens_without_stop_words %>%  group_by(word) %>% summarise(count=n(),articles=length(unique(id))) %>% arrange(desc(count))

```
co occorenze delle parole

https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html#2_counting_co-occurrences

```{r}

require(Matrix)
name<-"trump"  # specify the articles of whom
mainWord<-"trump"  # specify the word to consider as the main one

get_co_occ<-function(data,name,mainWord){
trumpArticles<-data %>% filter(who==name & present) %>% anti_join(data %>% filter(who=="both"),by="id")
text<-trumpArticles$article %>% tolower




ds  <- Corpus(VectorSource(text))
binDTM <- DocumentTermMatrix(ds, control=list(bounds = list(global=c(1, Inf)), weighting = weightBin))

tdm <- DocumentTermMatrix(ds)

frequentTerms <- findFreqTerms(tdm, lowfreq = 5) 
tdm<-NULL
binDTM<-binDTM[,frequentTerms] 



binDTM <- sparseMatrix(i = binDTM$i, j = binDTM$j, x = binDTM$v, dims = c(binDTM$nrow, binDTM$ncol), dimnames = dimnames(binDTM))

# Matrix multiplication for cooccurrence counts
coocCounts <- t(binDTM) %*% binDTM


cc<-as.matrix(coocCounts)


#Trova co occorrenze più frequenti

diag(cc)<-0# sulla diagonale si ha il nuero di volte che è comparsa una certa parola

# elimina parole poco usate
cwords<-words$word
ic<-which(colnames(cc)%in%words$word)
cc_s<-cc[ic,ic]
freq<-order(cc_s,decreasing = TRUE)
words<-colnames(cc_s)
#co_occ<-lapply(freq[1:50],function(x){return(c(words[floor(x/nrow(cc_s))],words[x%%nrow(cc_s)]))})

co_occ<-sapply(freq[1:50],function(x){return(paste(words[floor(x/nrow(cc_s))],words[x%%nrow(cc_s)],sep="-"))})

d<-data.frame(a=co_occ) %>% separate(a,sep = "-",into=c("a","b"))
l<-(d %>% group_by(a) %>% summarise(count=n())) %>% rename(w=a)
r<-(d %>% group_by(b) %>% summarise(count=n())) %>% rename(w=b)
co_occ<-l %>% rbind( r) %>% arrange(desc(count))


calculateCoocStatistics<-function(coocTerm,binDTM,measure){
  k <- nrow(binDTM)
  ki <- sum(binDTM[, coocTerm])
  kj <- colSums(binDTM)
  names(kj) <- colnames(binDTM)
  kij <- coocCounts[coocTerm, ]
  
  if(measure=="mutual"){
  ########## MI: log(k*kij / (ki * kj) ########
    mutualInformationSig <- log(k * kij / (ki * kj))
    mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]
    return(mutualInformationSig)
  }else if (measure=="dice"){
    ########## DICE: 2 X&Y / X + Y ##############
    dicesig <- 2 * kij / (ki + kj)
    dicesig <- dicesig[order(dicesig, decreasing=TRUE)]
    return(dicesig)
  }else{
    ########## Log Likelihood ###################
    logsig <- 2 * ((k * log(k)) - (ki * log(ki)) - (kj * log(kj)) + (kij * log(kij)) 
              + (k - ki - kj + kij) * log(k - ki - kj + kij) 
              + (ki - kij) * log(ki - kij) + (kj - kij) * log(kj - kij) 
              - (k - ki) * log(k - ki) - (k - kj) * log(k - kj))
    logsig <- logsig[order(logsig, decreasing=T)]
    return(logsig)
  }
}

term<-mainWord
kij <- coocCounts[term, ]
mutualInformationSig<-calculateCoocStatistics(term,binDTM,"mutual")
dicesig<-calculateCoocStatistics(term,binDTM,"dice")
logsig<-calculateCoocStatistics(term,binDTM,"mutual")
# Put all significance statistics in one Data-Frame
resultOverView <- data.frame(
  names(sort(kij, decreasing=T)[1:10]), sort(kij, decreasing=T)[1:10],
  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], 
  names(dicesig[1:10]), dicesig[1:10], 
  names(logsig[1:10]), logsig[1:10],
  row.names = NULL)
colnames(resultOverView) <- c("Freq-terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LL-Terms", "LL")
print(resultOverView)



coocTerm<-mainWord # specify the word to consider as the main one
coocs <- calculateCoocStatistics(term, binDTM, measure="LOGLIK")
numberOfCoocs<-15
resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
# The structure of the temporary graph object is equal to that of the resultGraph
tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
tmpGraph[, 1] <- coocTerm
# Entry of the co-occurrences into the second column of the respective line
tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]
# Set the significances
tmpGraph[, 3] <- coocs[1:numberOfCoocs]

# Attach the triples to resultGraph
resultGraph <- rbind(resultGraph, tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs){
  
  # Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newCoocTerm <- names(coocs)[i]
  coocs2 <- calculateCoocStatistics(newCoocTerm, binDTM, measure="LOGLIK")
  
  #print the co-occurrences
  coocs2[1:10]
  
  # Structure of the temporary graph object
  tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]
  tmpGraph[, 1] <- newCoocTerm
  tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]
  tmpGraph[, 3] <- coocs2[1:numberOfCoocs]
  
  #Append the result to the result graph
  resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])
}

resultGraph[sample(nrow(resultGraph), 6), ]




require(igraph)

# Set the graph and type. In this case, "F" means "Force Directed"
graphNetwork <- graph.data.frame(resultGraph, directed = F)

# Identification of all nodes with less than 2 edges
graphVs <- V(graphNetwork)[degree(graphNetwork) < 2]
# These edges are removed from the graph
graphNetwork <- delete.vertices(graphNetwork, graphVs) 

# Assign colors to edges and nodes (searchterm blue, rest orange)
V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, 'cornflowerblue', 'orange') 

# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange
halfMaxSig <- max(E(graphNetwork)$sig) * 0.5
E(graphNetwork)$color <- ifelse(E(graphNetwork)$sig > halfMaxSig, "coral", "azure3")

# Disable edges with radius
E(graphNetwork)$curved <- 0 
# Size the nodes by their degree of networking
V(graphNetwork)$size <- log(degree(graphNetwork)) * 5

# All nodes must be assigned a standard minimum-size
V(graphNetwork)$size[V(graphNetwork)$size < 5] <- 3 

# edge thickness
E(graphNetwork)$width <- 2

# Define the frame and spacing for the plot
par(mai=c(0,0,1,0)) 

# Finaler Plot
plot(graphNetwork,              
     layout = layout.fruchterman.reingold,  # Force Directed Layout 
     main = paste(coocTerm, ' Graph'),
     vertex.label.family = "sans",
     vertex.label.cex = 0.8,
     vertex.shape = "circle",
     vertex.label.dist = 0.5,           # Labels of the nodes moved slightly
     vertex.frame.color = 'darkolivegreen',
     vertex.label.color = 'black',      # Color of node names
     vertex.label.font = 2,         # Font of node names
     vertex.label = V(graphNetwork)$name,       # node names
     vertex.label.cex = 1 # font size of node names 
)
##########
#ggraph(graphNetwork)+
#  geom_node_point()+
#  geom_node_text()+
#  geom_edge_link()
return (V(graphNetwork)$name)
}
get_co_occ(data,name,mainWord)

```

```{r}
name<-"trump"
x<-get_co_occ(data,name,name)
```

```{r}
get_co_occ(data %>% filter(beforeEl),name,name)
```

```{r}
get_co_occ(data %>% filter(!beforeEl),name,name)


```



Topic modelling
```{r}
library(topicmodels)

name<-"clinton"  # specify the articles of whom


trumpArticles<-data %>% filter(who==name & present) %>% anti_join(data %>% filter(who=="both"),by="id")
text<-trumpArticles$article %>% tolower


tokens<-trumpArticles %>% mutate(article=removePunctuation(article)) %>%  unnest_tokens(word, article) %>% ungroup()


# rimuovi stop words
stop_words<-data.frame(word=stopwords("en"))
tokens_without_stop_words<-tokens %>% anti_join(stop_words)
words<-tokens_without_stop_words %>% group_by(datew,word) %>% summarise(count=n())


ss<-words %>%   cast_dtm(datew, word, count)

ap_lda <- LDA(ss, k = 2, control = list(seed = 1234))

ap_topics <- tidy(ap_lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


beta_spread <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

```

```{r}

name<-"clinton"  # specify the articles of whom

get_words<-function(data,name,tf_idf=T){
trumpArticles<-data %>% filter(who==name & present) %>% anti_join(data %>% filter(who=="both"),by="id")
text<-trumpArticles$article %>% tolower


tokens<-trumpArticles %>% mutate(article=removePunctuation(article)) %>%  unnest_tokens(word, article) %>% ungroup()


# rimuovi stop words
stop_words<-data.frame(word=stopwords("en"))
tokens_without_stop_words<-tokens %>% anti_join(stop_words)

if(tf_idf){
#consider tf-idf
words<-tokens_without_stop_words %>% group_by(datew,word) %>% summarise(count=n())
ww<-words %>%  bind_tf_idf(word, datew, count) %>% mutate(count=tf_idf)
} else{
  words<-tokens_without_stop_words %>% group_by(datew,word) %>% summarise(count=n())
  #word used for each person during th weeks
ww<-words %>% group_by(datew,word) %>% summarise(count=sum(count))# average usage of word

}
wa<-ww %>% group_by(word) %>% summarise(avg=mean(count)) %>% arrange(-avg)
return(wa)
}
w<-get_words(data,"trump",F)

w<-get_words(data,"trump",T)



wb<-get_words(data %>% filter(beforeEl),"trump",F)

wordcloud(w$word,w$avg,max.words = 50)

wa<-get_words(data %>% filter(!beforeEl),"trump",F)

plotRanks <- function(a, b, lab_a=a,lab_b=b,labels.offset=0.1, arrow.len=0.1){
  a<-rev(a)
  b<-rev(b)
  old.par <- par(mar=c(1,1,1,1))

  # Find the length of the vectors
  len.1 <- length(a)
  len.2 <- length(b)

  # Plot two columns of equidistant points
  plot(rep(1, len.1), 1:len.1, pch=20, cex=0.8, 
       xlim=c(0, 3), ylim=c(0, max(len.1, len.2)),
       axes=F, xlab="", ylab="") # Remove axes and labels
  points(rep(2, len.2), 1:len.2, pch=20, cex=0.8)

  # Put labels next to each observation
  text(rep(1-labels.offset, len.1), 1:len.1, lab_a)
  text(rep(2+labels.offset, len.2), 1:len.2, lab_b)

  # Now we need to map where the elements of a are in b
  # We use the match function for this job
  a.to.b <- match(a, b)

  # Now we can draw arrows from the first column to the second
  arrows(rep(1.02, len.1), 1:len.1, rep(1.98, len.2), a.to.b, 
         length=arrow.len, angle=20)
  par(old.par)
}

prev_o<-wb$word
aft_o<-wa$word
m<-abs(match(prev_o,aft_o)-seq(1,length(prev_o)))
ii<-order(m)[1:30]
prev<-prev_o[ii]
prev2<-paste(ii,prev)
f<-match(prev,aft_o)
aft<-aft_o[f]
aft2<-paste(f,aft[f])

ii<-order(match(prev,prev_o))
prev<-prev[ii]
prev2<-prev2[ii]

ii<-order(match(aft,aft_o))
aft<-aft[ii]
aft2<-paste(match(aft,aft_o),aft)
#plotRanks(prev,aft,prev2,aft2)


plotRanks(wb$word[1:30],wa$word[1:30])

wcb<-get_words(data %>% filter(beforeEl),"clinton",F)

#plotRanks(wb$word[1:30],wcb$word[1:30])


wca<-get_words(data %>% filter(!beforeEl),"clinton",F)




# show usage of words for trump and clinton during the weeks, in particular before and after the election
```


```{r}
trumpArticles<-data %>% filter((who=="trump" & present)|(who=="clinton" & present))
text<-trumpArticles$article %>% tolower


tokens<-trumpArticles %>% mutate(article=removePunctuation(article)) %>%  unnest_tokens(word, article) %>% ungroup()


# rimuovi stop words
stop_words<-data.frame(word=stopwords("en"))
tokens_without_stop_words<-tokens %>% anti_join(stop_words)
words<-tokens_without_stop_words %>% group_by(datew,word) %>% summarise(count=n())



ss<-words %>%   cast_dtm(datew, word, count)

ap_lda <- LDA(ss, k = 2, control = list(seed = 1234))

ap_topics <- tidy(ap_lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


beta_spread <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

```
